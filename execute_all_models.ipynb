{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f9c96ec",
   "metadata": {},
   "source": [
    "# Importação de bibliotecas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af04bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, GRU, Conv1D, MaxPooling1D,  Bidirectional\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import linear_model\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import statistics\n",
    "import statsmodels.api as sm \n",
    "from PyEMD import EMD, EEMD, CEEMDAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e801ef0",
   "metadata": {},
   "source": [
    "# Pasta indicada como raiz \n",
    "* O comando os.listdir retorna uma lista com o nome de todos os arquivos contidos na pasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2d8104",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = r'BASE_DADOS_MET'\n",
    "arquivos = os.listdir(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090028b7",
   "metadata": {},
   "source": [
    "# Montar Janela decomposição Sazonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58606223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def montarJ(data, janela, step,  trend, sazo, ruido):\n",
    "    \n",
    "    x, y = [],[]\n",
    "    \n",
    "    for i in range(janela, len(data)):\n",
    "        \n",
    "        fim_y = i+step\n",
    "        if fim_y > len(data):\n",
    "            break\n",
    "        a = np.array(data[i-janela:i,0])\n",
    "        t = trend[i-1:i,0]\n",
    "        s = sazo[i-1:i,0]\n",
    "        r = ruido[i-1:i,0]\n",
    "        \n",
    "        c = np.concatenate((a, t, s, r))\n",
    "        \n",
    "        \n",
    "        x.append(c)\n",
    "        \n",
    "        y.append(data[i:fim_y,0])\n",
    "        \n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8dde66",
   "metadata": {},
   "source": [
    "# Montar janela decomposição EEMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89df9ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def montarJ1(data, janela, step, IMF1, IMF2, IMF3, IMF4, IMF5, res1):\n",
    "    \n",
    "    x, y = [],[]\n",
    "    \n",
    "    for i in range(janela, len(data)):\n",
    "        \n",
    "        fim_y = i+step\n",
    "        if fim_y > len(data):\n",
    "            break\n",
    "        a = np.array(data[i-janela:i,0])\n",
    "        IM1 = IMF1[i-1:i,0] \n",
    "        IM2 = IMF2[i-1:i,0] \n",
    "        IM3 = IMF3[i-1:i,0] \n",
    "        IM4 = IMF4[i-1:i,0] \n",
    "        IM5 = IMF5[i-1:i,0]\n",
    "        rs1 = res1[i-1:i,0] \n",
    "        \n",
    "        c = np.concatenate((a, IM1, IM2, IM3, IM4, IM5, rs1))\n",
    "        \n",
    "        \n",
    "        x.append(c)\n",
    "        \n",
    "        y.append(data[i:fim_y,0])\n",
    "        \n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d444f6",
   "metadata": {},
   "source": [
    "# Montar janela dados sem decomposição "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47feccc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def montarJanela(data, janela, step):\n",
    "    \n",
    "    x, y = [],[]\n",
    "    \n",
    "    for i in range(janela, len(data)):\n",
    "        \n",
    "        fim_y = i+step\n",
    "        if fim_y > len(data):\n",
    "            break\n",
    "        \n",
    "        x.append(data[i-janela:i,0])\n",
    "        y.append(data[i:fim_y,0])\n",
    "        \n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f269490d",
   "metadata": {},
   "source": [
    "# Avalia modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3525be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliaModelo1(y_test, resul, nomeModel, REGIAO):\n",
    "    \n",
    "    tm = len(y_test.columns)\n",
    "    res = 0\n",
    "    RMSE = 0\n",
    "    EMA = 0\n",
    "    cor = 0\n",
    "    \n",
    "    print(\"------------------------------//-------------------------------------\")\n",
    "    print(f\"REGIAO - {REGIAO}\")\n",
    "    print(f\"Resultados {nomeModel}\")\n",
    "    if(tm > 1):\n",
    "        for i in range(len(y_test.columns)):\n",
    "            res = res + (metrics.r2_score(y_test.iloc[:, i], resul.iloc[:, i]))\n",
    "            cor = cor + (y_test.iloc[:, i].corr(resul.iloc[:, i]))\n",
    "            RMSE = RMSE + (math.sqrt(metrics.mean_squared_error(y_test.iloc[:, i], resul.iloc[:, i])))\n",
    "            EMA = EMA + (metrics.mean_absolute_error(y_test.iloc[:, i], resul.iloc[:, i]))\n",
    "        \n",
    "        print(f\"{nomeModel} - R2 = {res/tm}\")\n",
    "        print(f\"{nomeModel} - COR = {cor/tm}\")\n",
    "        print(f\"{nomeModel} - Raiz Erro Medio Quad (REMQ) = {RMSE/tm}\")\n",
    "        print(f\"{nomeModel} - Erro Medio Abs (EMA) = {EMA/tm}\")\n",
    "    else:\n",
    "        print(f\"{nomeModel} - R2 = {metrics.r2_score(y_test, resul)}\")\n",
    "        print(f\"{nomeModel} - COR = {y_test.iloc[:, 0].corr(resul.iloc[:, 0])}\")\n",
    "        print(f\"{nomeModel} - Raiz Erro Medio Quad (REMQ) = {math.sqrt(metrics.mean_squared_error(y_test, resul))}\")\n",
    "        print(f\"{nomeModel} - Erro Medio Abs (EMA) = {metrics.mean_absolute_error(y_test, resul)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2d4bdf",
   "metadata": {},
   "source": [
    "# Plota Gráficos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1d1ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotGraficoPR(real, prev):\n",
    "    tm = len(real.columns)\n",
    "    if(tm > 1):\n",
    "        fig, axs = plt.subplots(nrows = tm, figsize = (16, 12))\n",
    "        for i in range(len(real.columns)):\n",
    "            axs[i].plot(real.iloc[:, i], label = 'Dados Reais')\n",
    "            axs[i].plot(prev.iloc[:, i], label = 'Dados Previstos')\n",
    "\n",
    "    else:\n",
    "        plt.figure(figsize=(16, 5))\n",
    "        plt.plot(real, label = 'Dados Reais')\n",
    "        plt.plot(prev, label = 'Dados Reais')\n",
    "        #plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0724324",
   "metadata": {},
   "source": [
    "# Cria RNA CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74958cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criaRedeLSTM(x_train, y_train, x_test):\n",
    "    \n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "    \n",
    "    regressor = Sequential()\n",
    "    regressor.add(Conv1D(filters = 128, kernel_size = 2, activation = 'tanh', input_shape=(x_train.shape[1], 1)))\n",
    "    regressor.add(MaxPooling1D(pool_size = 1))\n",
    "    regressor.add(LSTM(units = 200, return_sequences = True, activation = 'tanh'))\n",
    "    regressor.add(Dropout(0.2))\n",
    "    regressor.add(LSTM(units = 50, activation = 'tanh'))\n",
    "    regressor.add(Dropout(0.1))\n",
    "    regressor.add(Dense(units = 25, activation = 'linear'))\n",
    "    regressor.add(Dropout(0.1))\n",
    "    regressor.add(Dense(y_train.shape[1], activation = 'linear'))\n",
    "    \n",
    "    regressor.compile(loss= 'mean_squared_error', optimizer= 'Adam', metrics=['mean_absolute_error'])\n",
    "    regressor.fit(x_train, y_train, epochs= 400, batch_size= 512, verbose=0)\n",
    "    \n",
    "    resul = regressor.predict(x_test)\n",
    "    \n",
    "    return resul"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfca8ca",
   "metadata": {},
   "source": [
    "# Cria Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2c3c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criaRDForest(x_train, y_train, x_test):\n",
    "    \n",
    "    regressor = RandomForestRegressor(n_estimators= 60, min_samples_split = 32, min_samples_leaf = 64, max_leaf_nodes = 72, max_depth = 9)\n",
    "    regressor.fit(x_train, y_train)\n",
    "    prev = regressor.predict(x_test)\n",
    "\n",
    "    return prev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40605795",
   "metadata": {},
   "source": [
    "# Cria árvore decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7604b523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criaARV(x_train, y_train, x_test):\n",
    "    regressor = DecisionTreeRegressor(min_samples_split = 512, min_samples_leaf = 128, max_leaf_nodes = 24, max_depth = 24) \n",
    "    regressor.fit(x_train, y_train)\n",
    "    prev = regressor.predict(x_test)\n",
    "\n",
    "    return prev\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d69b2bc",
   "metadata": {},
   "source": [
    "# Aplica Modelos com a decomposição sazonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78805dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "resultados = pd.DataFrame()\n",
    "\n",
    "cont = 1\n",
    "while (cont < len(arquivos)):\n",
    "    \n",
    "    #Monta o caminho com um arquivo específico \n",
    "    caminho = r\"\"+file+\"\\\\\"+arquivos[cont]\n",
    "    \n",
    "    #Abre o arquivo na pasta \n",
    "    df = pd.read_excel(caminho)\n",
    "    \n",
    "    \n",
    "    #atribui os dados a variável DTF\n",
    "    DTF = df[['PRECIPITACAO(mm) (ACM)']]\n",
    "\n",
    "    #VENTO_VEL_H (m/s) (MD)  PRECIPITACAO(mm) (ACM) UMID_REL_AR_H (%) (MD) TEMPMED (C) (MD)\n",
    "    \n",
    "    \n",
    "    # Muda a escala dos valores para 0 a 1\n",
    "    scaler = MinMaxScaler()\n",
    "    dataF = scaler.fit_transform(DTF)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Aplica a decomposição sazonal\n",
    "    uso de extrapolate_trend ajuda a garantir que a série de tendência, sazonalidade e ruído tenham o mesmo \n",
    "    tamanho que a série original.   extrapolate_trend = 5  método de decomposição tenta extrapolar a tendência \n",
    "    para além dos limites da série temporal, usando dados adicionais para suavizar o comportamento da tendência. \n",
    "    Com extrapolate_trend=5, por exemplo, a tendência é extrapolada usando 5 pontos de dados extras na borda da \n",
    "    série para suavização\"\"\"\n",
    "    \n",
    "    decomposicao = sm.tsa.seasonal_decompose(DTF, model='additive', period  = 3, extrapolate_trend = 5)\n",
    "    \n",
    "    #obtem tendencia, sazonalidade e o ruido \n",
    "    tendencia =  decomposicao.trend\n",
    "    sazonalidade = decomposicao.seasonal\n",
    "    ruido = decomposicao.resid\n",
    "    \n",
    "    print('Tamanho DTF: ',len(DTF))\n",
    "    print('Tamanho Tende ',len(tendencia))\n",
    "    print('Tamanho sazonalidade: ',len(sazonalidade))\n",
    "    \n",
    "    #Preenche a tendencia, sazonalidade de ruido com 0\n",
    "    tendencia = np.nan_to_num(tendencia)\n",
    "    sazonalidade = np.nan_to_num(sazonalidade)\n",
    "    ruido = np.nan_to_num(ruido)\n",
    "    \n",
    "    #Normaliza a tendência a sozonalidade e o ruído\n",
    "    trend = np.reshape(tendencia, (-1, 1))\n",
    "    sazo = np.reshape(sazonalidade, (-1, 1))\n",
    "    rd = np.reshape(ruido, (-1, 1))\n",
    "    \n",
    "    \n",
    "    scaler1 = MinMaxScaler()\n",
    "    trend = scaler1.fit_transform(trend)\n",
    "    \n",
    "    scaler2 = MinMaxScaler()\n",
    "    sazo = scaler2.fit_transform(sazo)\n",
    "    \n",
    "    scaler3 = MinMaxScaler()\n",
    "    rd = scaler3.fit_transform(rd)\n",
    "    \n",
    "    x, y = montarJ(dataF, 3, 1, trend, sazo, rd)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size= 0.3, random_state= 0, shuffle=False)\n",
    "    \n",
    "    \n",
    "    #regressor = RandomForestRegressor()\n",
    "    PREV_LSTM = criaRedeLSTM(X_train, y_train, X_test)\n",
    "    PREV_RF = criaRDForest(X_train, y_train, X_test)\n",
    "    PREV_ARV = criaARV(X_train, y_train, X_test)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Quando a previsão for de um único dia a frente é necessário um reshape dos dados previstos para reverter a escala, quando for de mais de um dia não é necessário  \n",
    "    \n",
    "    PREV_LSTM = np.reshape(PREV_LSTM, (-1, 1))\n",
    "    PREV_RF = np.reshape(PREV_RF, (-1, 1))\n",
    "    PREV_ARV = np.reshape(PREV_ARV, (-1, 1))\n",
    "\n",
    "    PREV_LSTM = scaler.inverse_transform(PREV_LSTM)\n",
    "    PREV_RF = scaler.inverse_transform(PREV_RF)\n",
    "    PREV_ARV = scaler.inverse_transform(PREV_ARV)\n",
    "    \n",
    "    \n",
    "    Real = scaler.inverse_transform(y_test)\n",
    "    \n",
    "    \n",
    "    #Somente quando for para prever mais de um passo a frente \n",
    "    Real = pd.DataFrame(Real)\n",
    "    PREV_LSTM = pd.DataFrame(PREV_LSTM)\n",
    "    PREV_RF = pd.DataFrame(PREV_RF)\n",
    "    PREV_ARV = pd.DataFrame(PREV_ARV)\n",
    "    \n",
    "    \n",
    "    avaliaModelo1(Real, PREV_LSTM, \"DECOMP_SD_LSTM\", arquivos[cont])\n",
    "    avaliaModelo1(Real, PREV_RF, \"DECOMP_SD_R_FOREST\", arquivos[cont])\n",
    "    avaliaModelo1(Real, PREV_ARV, \"DECOMP_SD_ARVORE\", arquivos[cont])\n",
    "    \n",
    "    resultados['DADOS_REAIS'] = Real[0]\n",
    "    resultados['DC_SD_LSTM'] = PREV_LSTM[0]\n",
    "    resultados['DC_SD_RF'] = PREV_RF[0]\n",
    "    resultados['DC_SD_ARV'] = PREV_ARV[0]\n",
    "  \n",
    "    nome = 'resultados_DC_AD'+'_'+arquivos[cont]\n",
    "    resultados.to_excel(nome)\n",
    "    \n",
    "    \n",
    "    cont = cont+3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cbf576",
   "metadata": {},
   "source": [
    "# Aplica Modelos com a decomposição EEMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31436dac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "resultados = pd.DataFrame()\n",
    "\n",
    "cont = 2\n",
    "\n",
    "while (cont < len(arquivos)):\n",
    "    \n",
    "    #Monta o caminho com um arquivo específico \n",
    "    caminho = r\"\"+file+\"\\\\\"+arquivos[cont]\n",
    "    \n",
    "    #Abre o arquivo na pasta \n",
    "    df = pd.read_excel(caminho)\n",
    "    \n",
    "    \n",
    "    #atribui os dados a variável DTF\n",
    "    DTF = df['UMID_REL_AR_H (%) (MD)'].values\n",
    "    #VENTO_VEL_H (m/s) (MD)  PRECIPITACAO(mm) (ACM) UMID_REL_AR_H (%) (MD) TEMPMED (C) (MD)\n",
    "    \n",
    "    DTF1 = np.reshape(DTF, (-1, 1))\n",
    "    \n",
    "    # Muda a escala dos valores para 0 a 1\n",
    "    scaler = MinMaxScaler()\n",
    "    dataF = scaler.fit_transform(DTF1)\n",
    "    \n",
    "    \"\"\"Os dados decompostos também serão inseridos nas janelas, mas apenas os valores correspondentes ao ponto que se \n",
    "    encontra na localização final da janela ex.: dados normais tamanho da janela = 3, incluir na janela a tendência, \n",
    "    sazonalidade e ruído no ponto correstondente a 3 em sua janela, ou seja, um único dado para cada \"\"\"\n",
    "    \n",
    "    t = np.linspace(0, 1, len(DTF))  # Tempo\n",
    "    \n",
    "    # Criar uma instância do EMD\n",
    "    eemd = EEMD()\n",
    "\n",
    "    # Realizar a decomposição\n",
    "    imfs = eemd.eemd(DTF, t)\n",
    "    \n",
    "    #Obtem o resíduo\n",
    "    res = DTF - np.sum(imfs, axis=0)\n",
    "\n",
    "    # Limite a 5 IMFs\n",
    "    imfs_limitadas = imfs[:5]\n",
    "\n",
    "    imf1 = imfs[0]\n",
    "    imf2 = imfs[1]\n",
    "    imf3 = imfs[2]\n",
    "    imf4 = imfs[3]\n",
    "    imf5 = imfs[4]\n",
    "    imf6 = imfs[5]\n",
    "    \n",
    "    #plt.plot(imf6)\n",
    "    #plt.show()\n",
    "    \n",
    "    \n",
    "    #Normaliza a tendência a sozonalidade e o ruído\n",
    "    IMF1 = np.reshape(imf1, (-1, 1))\n",
    "    IMF2 = np.reshape(imf2, (-1, 1))\n",
    "    IMF3 = np.reshape(imf3, (-1, 1))\n",
    "    IMF4 = np.reshape(imf4, (-1, 1))\n",
    "    IMF5 = np.reshape(imf5, (-1, 1))\n",
    "    #IMF6 = np.reshape(imf6, (-1, 1))\n",
    "    \n",
    "    # Aplica a normalização \n",
    "    scaler1 = MinMaxScaler()\n",
    "    IMF1 = scaler1.fit_transform(IMF1)\n",
    "    \n",
    "    scaler2 = MinMaxScaler()\n",
    "    IMF2 = scaler2.fit_transform(IMF2)\n",
    "    \n",
    "    scaler3 = MinMaxScaler()\n",
    "    IMF3 = scaler3.fit_transform(IMF3)\n",
    "    \n",
    "    scaler4 = MinMaxScaler()\n",
    "    IMF4 = scaler4.fit_transform(IMF4)\n",
    "    \n",
    "    scaler5 = MinMaxScaler()\n",
    "    IMF5 = scaler5.fit_transform(IMF5)\n",
    "    \n",
    "    #scaler6 = MinMaxScaler()\n",
    "    #IMF6 = scaler5.fit_transform(IMF6)\n",
    "    \n",
    "    res = np.reshape(res, (-1, 1))\n",
    "    scaler5 = MinMaxScaler()\n",
    "    res1 = scaler5.fit_transform(res)\n",
    "    \n",
    "    \n",
    "    # Monta a janela e divide o conjunto de dados\n",
    "    x, y = montarJ1(dataF, 3, 3, IMF1, IMF2, IMF3, IMF4, IMF5, res1)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size= 0.3, random_state= 0)\n",
    "    \n",
    "    \n",
    "    #Aplica os modelos de previsao\n",
    "    PREV_LSTM = criaRedeLSTM(X_train, y_train, X_test)\n",
    "    PREV_RF = criaRDForest(X_train, y_train, X_test)\n",
    "    PREV_ARV = criaARV(X_train, y_train, X_test)\n",
    "    \n",
    "    \n",
    "    #Quando a previsão for de um único dia a frente é necessário um reshape dos dados previstos para reverter a escala, quando for de mais de um dia não é necessário  \n",
    "    #PREV_LSTM = np.reshape(PREV_LSTM, (-1, 1))\n",
    "    #PREV_RF = np.reshape(PREV_RF, (-1, 1))\n",
    "    #PREV_ARV = np.reshape(PREV_ARV, (-1, 1))\n",
    "\n",
    "    \n",
    "    PREV_LSTM = scaler.inverse_transform(PREV_LSTM)\n",
    "    PREV_RF = scaler.inverse_transform(PREV_RF)\n",
    "    PREV_ARV = scaler.inverse_transform(PREV_ARV)\n",
    "    \n",
    "    \n",
    "    Real = scaler.inverse_transform(y_test)\n",
    "    \n",
    "    \n",
    "    #Somente quando for para prever mais de um passo a frente \n",
    "    Real = pd.DataFrame(Real)\n",
    "    PREV_LSTM = pd.DataFrame(PREV_LSTM)\n",
    "    PREV_RF = pd.DataFrame(PREV_RF)\n",
    "    PREV_ARV = pd.DataFrame(PREV_ARV)\n",
    "    \n",
    "    avaliaModelo1(Real, PREV_LSTM, \"DECOMP_EEMD_LSTM\", arquivos[cont])\n",
    "    avaliaModelo1(Real, PREV_RF, \"DECOMP_EEMD_R_FOREST\", arquivos[cont])\n",
    "    avaliaModelo1(Real, PREV_ARV, \"DECOMP_EEMD_ARVORE\", arquivos[cont])\n",
    "    \n",
    "    #resultados['DADOS_REAIS'] = Real[0]\n",
    "    #resultados['DC_EEMD_LSTM'] = PREV_LSTM[0]\n",
    "    #resultados['DC_EEMD_RF'] = PREV_RF[0]\n",
    "    #resultados['DC_EEMD_ARV'] = PREV_ARV[0]\n",
    "  \n",
    "    #nome = 'resultados_DC_EEMD'+'_'+arquivos[cont]\n",
    "    #resultados.to_excel(nome)\n",
    "    \n",
    "    cont = cont+3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d276c81f",
   "metadata": {},
   "source": [
    "# Aplica modelos sem decomposição "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b98848",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = pd.DataFrame()\n",
    "cont = 2\n",
    "\n",
    "while (cont < len(arquivos)):\n",
    "    \n",
    "    #Monta o caminho com um arquivo específico \n",
    "    caminho = r\"\"+file+\"\\\\\"+arquivos[cont]\n",
    "    \n",
    "    #Abre o arquivo na pasta \n",
    "    df = pd.read_excel(caminho)\n",
    "    \n",
    "    \n",
    "    #atribui os dados a variável DTF\n",
    "    DTF = df['PRECIPITACAO(mm) (ACM)'].values\n",
    "    #VENTO_VEL_H (m/s) (MD)  PRECIPITACAO(mm) (ACM) UMID_REL_AR_H (%) (MD) TEMPMED (C) (MD)\n",
    "    \n",
    "    DTF1 = np.reshape(DTF, (-1, 1))\n",
    "    \n",
    "    # Muda a escala dos valores para 0 a 1\n",
    "    scaler = MinMaxScaler()\n",
    "    dataF = scaler.fit_transform(DTF1)\n",
    "    \n",
    "    # Montar Janela\n",
    "    x, y = montarJanela(dataF, 3, 4)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size= 0.3, random_state= 0)\n",
    "    \n",
    "    \n",
    "    #regressor = RandomForestRegressor()\n",
    "    PREV_LSTM = criaRedeLSTM(X_train, y_train, X_test)\n",
    "    PREV_RF = criaRDForest(X_train, y_train, X_test)\n",
    "    PREV_ARV = criaARV(X_train, y_train, X_test)\n",
    "    \n",
    "    \n",
    "    #Quando a previsão for de um único dia a frente é necessário um reshape dos dados previstos para reverter a escala, quando for de mais de um dia não é necessário  \n",
    "    #PREV_LSTM = np.reshape(PREV_LSTM, (-1, 1))\n",
    "    #PREV_RF = np.reshape(PREV_RF, (-1, 1))\n",
    "    #PREV_ARV = np.reshape(PREV_ARV, (-1, 1))\n",
    "\n",
    "    \n",
    "    PREV_LSTM = scaler.inverse_transform(PREV_LSTM)\n",
    "    PREV_RF = scaler.inverse_transform(PREV_RF)\n",
    "    PREV_ARV = scaler.inverse_transform(PREV_ARV)\n",
    "    \n",
    "    \n",
    "    Real = scaler.inverse_transform(y_test)\n",
    "    \n",
    "    \n",
    "    #Somente quando for para prever mais de um passo a frente \n",
    "    Real = pd.DataFrame(Real)\n",
    "    PREV_LSTM = pd.DataFrame(PREV_LSTM)\n",
    "    PREV_RF = pd.DataFrame(PREV_RF)\n",
    "    PREV_ARV = pd.DataFrame(PREV_ARV)\n",
    "    \n",
    "    avaliaModelo1(Real, PREV_LSTM, \"SEM_DECOMP_LSTM\", arquivos[cont])\n",
    "    avaliaModelo1(Real, PREV_RF, \"SEM_DECOMP_R_FOREST\", arquivos[cont])\n",
    "    avaliaModelo1(Real, PREV_ARV, \"SEM_DECOMP_ARVORE\", arquivos[cont])\n",
    "    \n",
    "    #resultados['DADOS_REAIS'] = Real[0]\n",
    "    #resultados['SEM_DC_LSTM'] = PREV_LSTM[0]\n",
    "    #resultados['SEM_DC_RF'] = PREV_RF[0]\n",
    "    #resultados['SEM_DC_ARV'] = PREV_ARV[0]\n",
    "  \n",
    "    #nome = 'resultados_SEM_DC_'+arquivos[cont]\n",
    "    #resultados.to_excel(nome)\n",
    "    \n",
    "    cont = cont+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a1413d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfacf5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
